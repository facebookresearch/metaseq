
# Metaseq
A codebase for working with [Open Pre-trained Transformers](projects/OPT).

## Getting Started
Follow [setup instructions here](docs/setup.md) to get started.

### Documentation on workflows
* [Training](docs/training.md)
* [API](docs/api.md)

### Background Info
* [Background & relationship to fairseq](docs/history.md)
* [Chronicles of training OPT-175B](projects/OPT/chronicles/README.md)

## Support
If you have any questions, bug reports, or feature requests regarding either the codebase or the models released in the projects section, please don't hesitate to post on our [Github Issues page](https://github.com/facebookresearch/metaseq/issues).

Please remember to follow our [Code of Conduct](CODE_OF_CONDUCT.md).

## Contributing
We welcome PRs from the community!

You can find information about contributing to metaseq in our [Contributing](docs/CONTRIBUTING.md) document.

## The Team
Metaseq is currently maintained by the CODEOWNERS: Susan Zhang (@suchenzang), Stephen Roller (@stephenroller), Anjali Sridhar (@anj-s), Naman Goyal (@ngoyal2707), Punit Singh Koura (@punitkoura), Moya Chen (@moyapchen), and Christopher Dewan (@m3rlin45).


## License

The majority of metaseq is licensed under the MIT license, however portions of the project are available under separate license terms: 
* Megatron-LM is licensed under the Megatron-LM license (https://github.com/NVIDIA/Megatron-LM/blob/main/LICENSE

