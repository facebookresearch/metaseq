from collections import defaultdict
from dataclasses import dataclass, field
from typing import Callable, Dict, List, Optional, TypedDict, Union

from metaseq.data.datasets.shared_transformers import identity_transformer


class DatasetItemLogprob(TypedDict):
    token_id: Optional[str]
    logprob_score: float


class DatasetItem(TypedDict):
    """
    This is the format that is used by Metaseq as input for finetuning and
    inference.
    """
    src: str
    tgt: str

    top_logprobs: Optional[List[Dict[str, DatasetItemLogprob]]]
    tokens: Optional[List[str]]
    token_scores: Optional[List[float]]


class MetaseqInferenceOutputItemBeamResult(TypedDict):
    generated_text: str

    top_logprobs: List[Dict[str, DatasetItemLogprob]]
    tokens: List[str]
    token_scores: List[float]


class MetaseqInferenceOutputItem(TypedDict):
    """
    This is the format that Metaseq's inference component will produce.
    """
    instance_idx: int
    target_text: str
    prompt_text: str
    beam_results: List[MetaseqInferenceOutputItemBeamResult]


class OAITeacherGeneratedDatasetItemLogprobs(TypedDict):
    tokens: List[str]
    token_logprobs: List[float]
    text_offset: List[int]
    top_logprobs: List[Dict[str, float]]


class OAITeacherGeneratedDatasetItem(TypedDict):
    """
    This is the format that OpenAI teacher-generated datasets are in.
    """
    source: str
    """
    The prompt that was used to generate the text.
    """

    human: str
    """
    The ground truth human label that corresponds with this prompt.
    """

    text: str
    """
    The text generated by the OpenAI model.
    """

    index: int
    finish_reason: str
    logprobs: OAITeacherGeneratedDatasetItemLogprobs


@dataclass
class DatasetTeacherGeneratedDataHooks:

    before_transforming_into_metaseq_inference: Callable[[OAITeacherGeneratedDatasetItem],
                                                         OAITeacherGeneratedDatasetItem] = field(
                                                             default_factory=lambda: identity_transformer
                                                         )
    """
    This hook will get called on each item of the dataset just before it is
    transformed into the format that metaseq expects for inference.

    For example, this hook will get executed when we're converting the OpenAI
    teacher-generated dataset into the format that Metaseq expects as input for
    inference/finetuning, or when we're converting it to inference format with
    the purpose of evaluating the teacher performance.
    """

    convert_test_target_to_original_domain_label: Callable[[str], str] = field(default_factory=lambda: identity_transformer)
    """
    Called when we're converting the test set into the format that Metaseq
    expects as input for inference/finetuning. This hook will get called on the
    target text created by the teacher so that it is converted into the same
    domain as the human labels. Note that this will get called AFTER
    `before_transforming_into_metaseq_inference`.

    For example, if the human labels are numbers like "1", "2", "3", but the
    model outputs numbers as words "one", "two", "three", then this hook will
    get called to convert the model output into actual numbers.
    """


@dataclass
class DatasetModelHooks:
    convert_model_type_output_to_original_domain: Dict[str, Callable[[str], str]] = field(
        default_factory=lambda:
        # by default every model type just retuns the identity transformer
        defaultdict(lambda: identity_transformer)
    )
    """
    This dict maps the model type to a function that will convert the model's output
    into the same domain as the original dataset.

    For example:
    {
        "distilled": lambda x: int(s.plit(" ")[0]),
    }
    """


class IdentityDict(defaultdict):
    """
    Wrapper around default dictionary that may be initialized with static keys.
    The dictionary maps function names to the function implemenation where
    undefined keys map to identity function.
    """

    def __init__(self, values: dict) -> None:
        super().__init__(lambda: identity_transformer)
        self.update(values)


@dataclass
class CommonDatasetConfiguration:
    metric_libraries: Optional[List[str]] = None
    """
    The list of metric libraries that will be used to evaluate the model's
    output
    """

    metric_names: Optional[List[str]] = None
    """
    The list of metric names that will be used to evaluate the model's output
    for this dataset.
    """


@dataclass
class DatasetDataHooks:
    ...


@dataclass
class DatasetConfigurationOriginal:
    """
    Configuration for an Original dataset (the one with human labels).
    """
    data_hooks: DatasetDataHooks = field(default_factory=lambda: DatasetDataHooks())


@dataclass
class DatasetConfigurationTeacherGenerated:
    """
    Configuration for a dataset that comes from a teacher-generated output.
    """
    data_hooks: DatasetTeacherGeneratedDataHooks = field(default_factory=lambda: DatasetTeacherGeneratedDataHooks())


@dataclass
class DatasetModelConfig:
    """
    Configuration for a model for a dataset.
    """
    model_hooks: DatasetModelHooks = field(default_factory=lambda: DatasetModelHooks())


@dataclass
class DatasetConfiguration:
    """
    Top level type for a dataset configuration. Includes configuration for the
    models, and different variations of the dataset (teacher generated, orig),
    etc.
    """
    common: CommonDatasetConfiguration
    model_config: DatasetModelConfig = field(default_factory=lambda: DatasetModelConfig())

    teacher_generated_config: DatasetConfigurationTeacherGenerated = field(
        default_factory=lambda: DatasetConfigurationTeacherGenerated()
    )
    orig_config: DatasetConfigurationOriginal = field(default_factory=lambda: DatasetConfigurationOriginal())
